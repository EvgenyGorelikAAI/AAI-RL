{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Actor Critic implementations\n",
    "\n",
    "Code from:\n",
    "\n",
    "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gymnasium as gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "num_steps = 300\n",
    "max_episodes = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        policy_dist = F.softmax(self.actor_linear2(policy_dist), dim=1)\n",
    "\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aai/anaconda3/envs/q-learning/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('highway-fast-v0')\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "# lets pass just 6 closest vehicles to the network and shift everything to cuda \n",
    "mask_observations = lambda o: torch.Tensor(o[:6]).flatten()\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "ac_optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "all_lengths = []\n",
    "average_lengths = []\n",
    "all_rewards = []\n",
    "entropy_term = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m values \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m rewards \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m steps \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[1;32m      8\u001b[0m     value, policy_dist \u001b[39m=\u001b[39m actor_critic\u001b[39m.\u001b[39mforward(state)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "for episode in range(max_episodes):\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "    for steps in range(num_steps):\n",
    "        value, policy_dist = actor_critic.forward(state)\n",
    "        value = value.detach().numpy()[0,0]\n",
    "        dist = policy_dist.detach().numpy() \n",
    "\n",
    "        action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
    "        log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "        entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "        new_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "        entropy_term += entropy\n",
    "        state = new_state\n",
    "        \n",
    "        if done or truncated:\n",
    "            Qval, _ = actor_critic.forward(new_state)\n",
    "            Qval = Qval.detach().numpy()[0,0]\n",
    "            all_rewards.append(np.sum(rewards))\n",
    "            all_lengths.append(steps)\n",
    "            average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "            if episode % 10 == 0:                    \n",
    "                sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "            break\n",
    "    \n",
    "    # compute Q values\n",
    "    Qvals = np.zeros_like(values)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + GAMMA * Qval\n",
    "        Qvals[t] = Qval\n",
    "\n",
    "    #update actor critic\n",
    "    values = torch.FloatTensor(values)\n",
    "    Qvals = torch.FloatTensor(Qvals)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    \n",
    "    advantage = Qvals - values\n",
    "    actor_loss = (-log_probs * advantage).mean()\n",
    "    critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "    ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "    ac_optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    ac_optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "plt.plot(all_rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(all_lengths)\n",
    "plt.plot(average_lengths)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode length')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cf68cbc5620f2f37a274bb61f6d37757583ee635f521547df98da585de52630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
